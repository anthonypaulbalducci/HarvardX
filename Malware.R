if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(caretEnsemble)) install.packages("caretEnsemble", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(caretEnsemble) # Library allows easy generation of ensembles with caret
library(ggplot2)
library(dplyr)
library(Metrics)
library(e1071) # The support vector machine library

malware_data <- read.csv("https://raw.githubusercontent.com/anthonypaulbalducci/HarvardX/main/Security/TUANDROMD.csv")

##################
# Data Wrangling #
##################

# Randomly shuffle the data 5 times as whole data set will be processed.
# I noticed some seeming collation in the way the final results were laid out
# and they say a casino shuffling the cards 4-7 times is sufficient.

for (x in 1:5) {
malware_data = malware_data[sample(1:nrow(malware_data)), ]
print(paste("Shuffle:",x))
}

# During my first run at analysis I was made aware of the fact that certain columns
# only have a single factor attribute, meaning they both prevent further analysis 
# and otherwise add no 'information' to the equation-- thus these columns are stripped
# and removed.

malware_data<- malware_data[, sapply(malware_data, function(col) length(unique(col))) > 1]
#malware_test_set <- malware_test_set[, sapply(malware_test_set, function(col) length(unique(col))) > 1]

sum(is.na(malware_data))

# Further a run over the data shows we are potentially dealing with a number of blank
# entries or 'NA's in the data set. From this examination we are looking at 241 columns 
# with potential candidates:

names(which(colSums(is.na(malware_data)) > 0))

# Interestingly, creating a frequency table() each of the 241 columns with an NA has
# only one entry. Perhaps it is a particular row that is affected ?

table(names(which(rowSums(is.na(malware_data)) > 0)))

# Okay, so luckily it appears only two rows are affected: 1 and 2534. Let's take a look.

malware_data[1,]
malware_data[2534,]

# Hmm, it is not immediately obvious why these two rows are so affected, but out of
# nrow(malware_data) I feel it is safe to just remove them rather than insert some
# potentially confusing offset value that affects the data or the total number of factors

malware_data <- na.omit(malware_data)

# Now double check:

sum(is.na(malware_data)) # sum is '0'-- no more NA's present

# Create train and test sets.
test_index <- createDataPartition(y = malware_data$Label, times = 1, p = 0.1, list = FALSE)
malware_train_set <- malware_data[-test_index,]
malware_test_set <- malware_data[test_index,]

print("Commencing runtime analysis")
start_time <- Sys.time() # Run-time analysis start

####################
# Perform analysis #
####################

set.seed(123, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier

malware_test_set$Label <- factor(malware_test_set$Label) # set test Label data to factor

trainControl (verboseIter = TRUE, number = 5, method = "cv") 
fit <- train(Label ~ ., method = "svmLinear", data = malware_train_set, trControl = trainControl())
end_time <- Sys.time() # Run-time analysis end

print(paste("Elapsed Analysis Runtime:", end_time - start_time)) # Run-time of model analysis displayed

results <- predict(fit, malware_test_set)

correct_prediction_1 <- 0
incorrect_prediction_1 <- 0

  for(i in 1:length(results)) {
    if (isTRUE(results[i] == malware_test_set$Label[i])) {
      correct_prediction_1 <- correct_prediction_1 + 1
    } else {
       incorrect_prediction_1 <- incorrect_prediction_1 + 1
     }
  }

print(paste("Correct predictions:",correct_prediction_1))
print(paste("Incorrect predictions:",incorrect_prediction_1))

print(confusionMatrix(results, malware_test_set$Label))

# 99% accuracy ! Not bad. Both quite high on sensitivity and specificity.
# Per course requirements we are supposed to test with a second algorithm;
# However the SVM did such a good job, I can only possibly expect the next
# model to do worse. But let's see !

trainControl (verboseIter = TRUE, number = 5, method = "cv") 
fit <- train(Label ~ ., method = "knn", data = malware_train_set, trControl = trainControl())
end_time <- Sys.time() #in Run-time analysis end

print(paste("Elapsed Analysis Runtime:", end_time - start_time)) # Run-time of model analysis displayed

results <- predict(fit, malware_test_set)

correct_prediction_2 <- 0
incorrect_prediction_2 <- 0

for(i in 1:length(results)) {
  if (isTRUE(results[i] == malware_test_set$Label[i])) {
    correct_prediction_2 <- correct_prediction_2 + 1
  } else {
    incorrect_prediction_2 <- incorrect_prediction_2 + 1
  }
}

print(paste("Correct predictions:",correct_prediction_2))
print(paste("Incorrect predictions:",incorrect_prediction_2))

print(confusionMatrix(results, malware_test_set$Label))



